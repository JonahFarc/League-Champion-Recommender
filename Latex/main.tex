\documentclass [11pt]{IEEEtran}
\usepackage[margin=1in]{geometry}
\usepackage{mathtools, amsmath, amssymb, upgreek}
\title{League of Legends Champion Recommendation System Using Collaborative Filtering\\ {\large CS 6375.003}}
\author{Tiffany Do, \and Dylan Yu, \and Salman Anwer}

\begin{document}
\maketitle
\section{Introduction}
League of Legends, originally released in 2009 by Riot Games, is the most widely played game worldwide \cite{gaudiosi_2013}. There is an estimated 100 million active players each month as of 2016 \cite{volk_2016}. The game currently has 141 playable characters, known as champions, from which players can choose from at the start of every match \cite{champlist}. These champions have widely varied play styles and abilities that allow a player to truly customize their way of playing League of Legends. With so many available champions, we wanted to create a recommendation system that would enable players to discover other champions they might like based on their current champion preferences.

\section{Problem Definition and Algorithm}
\subsection{Task Definition}
The input to our system is a player’s unique username, known as a summoner name. We then pull this player’s champion preferences, compare these preferences with our model, and output five recommended champions.
\subsection{Algorithm Definition}
We built our champion recommender using a collaborative filtering approach \cite{Sarwar:2001}, specifically the SVD algorithm (singular value decomposition). 
This domain is suitable for collaborative filtering as there is a massive base of users with diverse tastes and high user engagement, thus avoiding the common issues of cold starts, rating sparsity, and user homogeneity.
The SVD algorithm provides low rank latent factor discovery and user-champion mapping onto this latent space, thereby mitigating popularity bias and strongly tailoring champion recommendations to a user's specific taste.
Furthermore, its success in a domain with similar characteristics, Netflix recommendations, proved promising \cite{paterek2007improving}.
We utilized a Python implementation of the SVD algorithm in the Surprise library \cite{Surprise}. \medskip
This implementation calculates a prediction $\hat{r}_{ui}$ using the following equation:
$$
    \hat{r}_{ui} = \mu + b_u + b_i + q_i^Tp_u
$$
Where if user $u$ is unknown, then the bias $b_u$ and factors $p_u$ are zero. The same applies for item $i$ and $b_i$ and $q_i$. The estimations are done by minimizing the squared error function
\cite{koren2009matrix}:
$$
    \min_{p*,q*,b*} \sum_{r_{ui} \in R_{train}} \left(r_{ui} - \hat{r}_{ui} \right)^2 + 
\lambda\left(b_i^2 + b_u^2 + ||q_i||^2 + ||p_u||^2\right)
$$
which is done by stochastic gradient descent:
\begin{align*}
b_u &\leftarrow b_u + \gamma (e_{ui} - \lambda b_u)\\
b_i &\leftarrow b_i + \gamma (e_{ui} - \lambda b_i)\\
p_u &\leftarrow p_u + \gamma (e_{ui} \cdot q_i - \lambda p_u)\\
q_i &\leftarrow q_i + \gamma (e_{ui} \cdot p_u - \lambda q_i)
\end{align*}
where $e_{ui} = r_{ui} - \hat{r}_{ui}$. Baselines are initialized to zero and user and item factors are randomly distributed according to normal distribution \cite{Surprise}.
\section{Experimental Evaluation}
\subsection{Methodology}
\subsubsection{Building a dataset}
SVD uses ratings of users in order to build a model that can provide recommendations \cite{koren2009matrix}. While we do not have explicit ratings of how much a player enjoys a champion, we are able to determine their preferences by how often they play a champion. We pulled champion preferences of 2514 random active\footnote{User has played within the last month} players using Riot’s official API \cite{riotapi} in order to train our model. These champion preferences are represented as Champion Mastery Points $(C_{MP})$, which is an integer measure of how much a player has played a certain champion. A player has a $C_{MP}$ for each champion in the game, where: 
$$
0 \ \leq C_{MP} \ \textless \ \infty
$$
Every time a player selects a champion for a game, the $C_{MP}$ for that champion increases by an integer value depending on their performance. There is no way to decrease $C_{MP}$. 
The use of $C_{MP}$ serves our aims well, as behaviorally observed preferences are more indicative of true preferences in this domain, and we can easily gain access to large datasets as Riot's API \cite{riotapi} allows us to grab each player's mastery points of all champions. We used this dataset of 2514 random players and their respective $C_{MP}$ values to build our recommender. However, because $0 \ \leq C_{MP} \ \textless \ \infty$, a user that plays the game more frequently would have a higher overall $C_{MP}$ than a user who plays less frequently. To combat this, we normalized these scores to a scale of 1-100, with a user's most played champion rated as a 100, regardless of their actual $C_{MP}$ value. Our dataset and code is available at \cite{github}. 
\subsubsection{Finding recommendations}
We queried the system with a player's summoner name and used Riot's API in order to find their player ID. We then used this ID to find their top five champions sorted by $C_{MP}$. We selected only the top five champions as $C_{MP}$ are not explicit ratings and a low $C_{MP}$ does not necessarily mean that a player does not enjoy a champion. We used these five $C_{MP}$ as the user's preferences to find each $\hat{r}_{ui} : \forall i \in Champions$. Below is an example of a sample player's top five $C_{MP}$ and their normalized $C_{MP}$ ratings. \\\\
\begin{tabular}{| l | l | l |}
    \hline
    Champion & $C_{MP}$ & Normalized $C_{MP}$ \\ \hline
    Nami & 367,191 & 100\\ 
    Zyra & 136,709 & 38 \\
    Cassiopeia & 106,064 & 29 \\
    Janna & 89,306 & 25\\
    Lulu & 59,486 & 17\\
    \hline
\end{tabular}
\vspace{.2cm} \\ \noindent
After the system calculates each $\hat{r}_{ui}$, it sorts these scores and outputs the top five respective champions as recommendations. 

\subsection{Results}
%Because our system gives recommendations, we do not have an explicit accuracy rating. 
Although we will evaluate the theoretical accuracy of our model in section IV, we were primarily interested in how practical our recommendations are. To do this,

we ran a preliminary user study with a sample size of $n=30$ among volunteers polled from LOLUTD (League of Legends at UTD). Our study was designed to ask a user their perceived champion ratings for 10 champions. 5 of which are pulled from our systems top recommendations, and 5 which are randomly picked from the remaining pool of champions. The placement of the questions is randomly determined to avoid introducing bias. We then analyzed the mean ratings given by the user. Our hypothesis is that the system recommendations score significantly higher than a random recommendation. Below is an example of a survey given to a user:
\newpage
\begin{flushleft} 
\textit{* indicates a random recommendation. This is not visible to the subject}\\
%\centering
\begingroup
\setlength{\tabcolsep}{10pt}
\renewcommand{\arraystretch}{1.5} 
\begin{tabular}{|l r|}
    \hline
    \multicolumn{2}{|p{.9\linewidth}|}{
    How would you rate the enjoyability of each champion from 1-10?} \\
    \multicolumn{2}{|p{.9\linewidth}|}{1 = I do not like playing this champion} \\
    \multicolumn{2}{|p{.9\linewidth}|}{10 = This champion is very enjoyable to play} \\
     1. Karma & \rule{1cm}{1pt} \\ 
     2. Graves *  & \rule{1cm}{1pt} \\
     3. Jax * & \rule{1cm}{1pt} \\
     4. Sona & \rule{1cm}{1pt} \\
     5. Zilean & \rule{1cm}{1pt} \\
     6. Syndra * & \rule{1cm}{1pt} \\
     7. Thresh & \rule{1cm}{1pt} \\
     8. Warwick * & \rule{1cm}{1pt} \\
     9. Zed & \rule{1cm}{1pt} \\ 
     10. Draven * & \rule{1cm}{1pt} \\
     \hline
\end{tabular}
\endgroup
\end{flushleft}
\noindent We ran a statistical test of significance using a one-sided 2-Sample T-test on our data. Below are our stated hypotheses:  
\begin{align*}
H_0: \mu = \mu_{0} \\
H_a: \mu > \mu_{0} 
\end{align*}
where $\mu$ is the mean of the recommended set scores and $\mu_{0}$ is the mean of the random set scores. \\
Our dataset has values of: 
\begin{align*}
\mu& = 6.4567 \\
\sigma& = 2.18286 \\
\mu_{0}& = 5.18 \\
\sigma_{0}& = 2.2340 \\
n = n_{0}& = 30
\end{align*}
where naught values correspond to the random set statistics.
After running our one-sided 2-Sample t-Test, we found that $t=2.2387$ and $p=0.01451$. With $ \alpha = 0.05$, we reject $H_0$ in favor of $H_a$. Our results are statistically significant. Our user study results are available at \cite{github}. 

\section{Discussion}
\subsection{Parameter Tuning}
The recommender tested in our user study made use of the naive SVD model in the Suprise library. 
The default parameters are the following \cite{Surprise}: 
\begin{align*}
    epoch& = 20 \\
    \lambda&  = 0.005 \\
    \upeta& = 0.02
\end{align*}
where $\lambda$ is the regularization term and $\upeta$ is the learning rate.
The results obtained in the user study indicate that the SVD model is appropriate for a champion recommender system. 
Following this practical validation, we tuned these model parameters using a grid search to maximize theoretical accuracy by minimizing RMSE \cite{Surprise}. 
The optimal parameters found are the following:
\begin{align*}
    epoch& = 20 \\
    \lambda& = 0.4 \\
    \upeta& = 0.0005
\end{align*}
By using these parameters for model fitting instead, stronger results in a user study would likely be obtained. 

\subsection{Exploration of other algorithms}
We built models using the Slope One \cite{lemire2005slope} and the SVD++ algorithms in the Surprise library and obtained unpromising results.
We found Slope One to be highly susceptible to popularity bias; popularly played champions such as Ezreal, Lee Sin and Yasuo were consistently recommended for users regardless of user preferences. 
The SVD++ model has a time complexity too large for our project. 
The Surprise library documentation gives an example runtime for a dataset consisting of 1 million rows. 
The runtimes for SVD and SVD++ were 0:02:13 and 2:54:19 \cite{Surprise} respectively for this dataset. 
For our dataset of 311,727 rows, a single run took over an hour.

\subsection{Domain Observations}
In League of Legends, the current most popular in-game strategy is known as a "meta" \cite{kou2014governance}. The meta changes frequently, causing different champions to fall in and out of favor.
$C_{MP}$ is affected by this changing meta, since certain champions may be picked more or less often.
However, since the data for our model is representative of champion ratings over the history of the game, rather than of any specific meta, we posit that the effects on $C_{MP}$ by meta are negligible. 
Another consideration is the fact that champions themselves may change over the course of the game's history, with some of these champion reworks completely changing user's preferences regarding the champion.
As $C_{MP}$ is maintained regardless of any changes to the champions, our model will be misled for some user preferences \cite{champUpdate}.

\section{Future Work}
Although our results are significant, they are also preliminary. It is important to note that the user study conducted is not representative of the overall population and further work will be needed to practically evaluate our recommender. 
While we believe that our results are promising, it would be important to run a rigorous study to see if  users actually enjoy their champion recommendations.
\\ \indent The previously discussed issues in \textit{Domain Observations} cannot be solved with the current level of access provided by Riot's API.
A solution to these issues would require a more granular access to user data.

\nocite{*}
\bibliographystyle{IEEEtran}
\bibliography{annot}
\end{document}
